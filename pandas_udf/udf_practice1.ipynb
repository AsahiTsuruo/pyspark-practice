{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://qiita.com/taka_yayoi/items/b65197128ee698d87910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### series to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_func(a: pd.Series, b: pd.Series)->pd.Series:\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute the function as pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    4\n",
       "2    9\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = pd.Series([1,2,3])\n",
    "display(multiply_func(x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the spark udf and execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply = pandas_udf(multiply_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## series iterator to series iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, pandas_udf, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def plus_one(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in batch_iter:\n",
    "        yield x + 1\n",
    "#https://www.sejuku.net/blog/23716\n",
    "#ジェネレータメモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(plus_one(col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bc = spark.sparkContext.broadcast(1)\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    y = y_bc.value  # initialize states\n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x + y\n",
    "    finally:\n",
    "        pass  # release resources here, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|plus_y(x)|\n",
      "+---------+\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(plus_y(col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple series iterator to series iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def multiply_two_cols(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, x)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(multiply_two_cols(\"x\", \"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### series to scaler udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mean_udf(v)|\n",
      "+-----------+\n",
      "|        4.2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdfd2cb5b443fb54ce2327695bf24ff2220c4ebfb3729c0ed64ac908f741e365"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('pyspark': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
